<!DOCTYPE html>
<html>

<head>
    <title>Project Dory Report</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="widdh=device-widdh, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='css/stylesReport.css') }}">

    

</head>

<body>
    <div class="header">
        <h1>Project Dory</h1>
        <h3>COMP 3106</h3>
        <h3>Team Members</h3>
    </div>

    <nav class="nav-bar">
        <dl>
            <dt><a href="#section1" class="title">1. Introduction</a></dt>   
            <dd><a href="#section1-1" class="subtitle">1.1 Background & Motivation</a></dd>
            <dd><a href="#section1-2" class="subtitle">1.2 Prior Work</a></dd>
            <br>
            <dt><a href="#section2">2. Methods</a></dt>
            <dd><a href="#section2-1" class="subtitle">2.1 Methods from AI</a></dd>
            <dd><a href="#section2-2" class="subtitle">2.2 Dataset</a></dd>
            <dd><a href="#section2-3" class="subtitle">2.3 Validation Strategy</a></dd>
            <br>
            <dt><a href="#section3">3. Results</a></dt>
            <dd><a href="#section3-1" class="subtitle">3.1 Qualitative results</a></dd>
            <dd><a href="#section3-2" class="subtitle">3.2 Quantitative results</a></dd>
            <br>
            <dt><a href="#section4">4. Results</a></dt>
            <dd><a href="#section4-1" class="subtitle">4.1 Limitations</a></dd>
            <dd><a href="#section4-2" class="subtitle">4.2 Implications</a></dd>
            <br>
            <dt><a href="#section5" class="title">5. Statement of contributions</a></dt>
            <dt><a href="#section6" class="title">6. References</a></dt>
        </dl>

        
        
    </nav>

    <section id="section1" class="title">
        <h1>1 Introduction</h1>

        <section id="section1-1" class="subtitle">
            <h3>1.1 Background & Motivation</h3>
            <p>
                For decades, researchers have been collecting sound data from the Earth's oceans without an efficient way to classify it. 
                NOAA (National Oceanic and Atmospheric Administration) and the U.S NAVY have collected over 300 terabyts of oceanic sound data over a 4 year period through the SanctSound project 
                which aims to allow people to easily explore and access as much of this data as possible. 
                Classifying this data is already happening thanks to the NOAA Big Data Program. 
                Project Dory hopes to build on this effort by providing a platform that is able to listen to and identify sounds created by marine mammals. 
                Inspired by Dr. Roger Paynes multi-platinum album Songs of The Humpback Whale, we hope this project will be used in the effort to preserve and study marine life.
            </p>
            <p>
                The objective of Project Dory is to develop a platform to aid researchers in classifying marine mammals from sound data. 
                We aim to develop an AI model that is able to classify the presence of one or multiple marine mammals inside a sound clip. 
                The platform will be able to take an audio file (.wav) and output what animals are present with a time stamp corresponding to the original audio file.
            </p>
        </section>

        <section id="section1-2" class="subtitle">
            <h3>1.2 Prior Work</h3>
            <p>Present the results obtained from the project.</p>
        </section>
    </section>

    <section id="section2" class="title">
        <h1>2 Methods</h1>

        <section id="section2-1" class="subtitle">
            <h3>2.1 Methods from artificial intelligence used</h3>
            <p>
                Project Dory utilizes deep learning techniques, such as convolutional neural networks (CNNs), 
                and classifying data using The Mel-frequency cepstral coefficients (MFCCs) to classify 
                the features of the audio data based on time and frequency. The AI model is trained using 
                a supervised learning approach, where it is trained on labeled marine mammal sound data
            </p>

            <p>
                &nbsp;<h3>CNN (Convolutional Neural Network)</h3>
                &nbsp;<img src="{{url_for('static', filename='CNN.png')}}" width="500">
                &nbsp;<p>WHAT?</p><p>A Convolutional Neural Network (CNN) is a specialized type of neural network designed for image processing and pattern recognition. </p>
                &nbsp;<p>HOW?</p><p>It employs convolutional layers that use filters to detect spatial patterns, preserving hierarchical relationships in the data. Pooling layers help reduce spatial dimensions, and fully connected layers interpret the learned features for final classification</p>
                &nbsp;<p>WHY?</p><p>...</p>

                &nbsp;<h3>MFCC (Mel-Frequency Cepstral Coefficients)</h3>
                &nbsp;<img src="{{url_for('static', filename='MFCC.png')}}" width="500">
                &nbsp;<p>WHAT?</p><p>The primary purpose of the MFCC function in Project Dory is to extract relevant features from the raw audio data</p>
                &nbsp;<p>HOW?</p><p>The MFCC process involves dividing the audio signal into short overlapping frames. For each frame, the power spectrum is calculated, and then a series of filter banks are applied in the Mel-frequency scale.</p>
                &nbsp;<p>WHY?</p><p>MFCCs are robust in the presence of background noise, making them suitable for real-world scenarios where marine environments can be acoustically challenging.</p>

                &nbsp;<h3>Supervised Learning</h3>
                &nbsp;<img src="{{url_for('static', filename='TRAIN.png')}}" width="500">
                &nbsp;<p>WHAT?</p><p>Supervised learning is a machine learning paradigm where the algorithm learns from labeled training data, making predictions based on input-output pairs</p>
                &nbsp;<p>HOW?</p><p>During training, the algorithm iteratively adjusts its parameters to minimize the difference between predicted and actual outputs. The model generalizes this learning to make accurate predictions on new, unseen data.</p>
                &nbsp;<p>WHY?</p><p></p>

                &nbsp;<h3>Algorithm/Optimizer</h3>
                &nbsp;<img src="{{url_for('static', filename='MAIN.png')}}" width="500">
                &nbsp;<p>WHAT?</p><p>The Adam optimizer is an optimization algorithm used in training neural networks, combining adaptive learning rates and momentum</p>
                &nbsp;<p>HOW?</p><p>It adapts learning rates for each parameter individually and incorporates momentum to accelerate convergence. It maintains separate adaptive learning rates for each parameter, dynamically adjusting them during training</p>
                &nbsp;<p>WHY?</p><p></p>
            </p>

        </section>

        <section id="section2-2" class="subtitle">
            <h3>2.2 Dataset</h3>
            <p>
                Project Dory plans on using the Watkins Marine Mammal Sound Database. 
                This database contains the labeled recordings of over 60 different marine mammals collected over 7 decades.
            </p>
        </section>

        <section id="section2-3" class="subtitle">
            <h3>2.3 Validation strategy or experiments</h3>
            <p>
                Project Dory will validate its findings through the introduction of new data not seen by the model. 
                We will perform loss validation calculations to see how well our model is able to predict the marine mammals present in the audio clip. 
                Once our model archives reasonable performance levels, 
                we will continue with the validation of labeled data by comparing our ground truth labels with the outputs from our model.
            </p>
        </section>
    </section>

    <section id="section3" class="title">
        <h1>3 Results</h1>

        <section id="section3-1" class="subtitle">
            <h3>3.1 Qualitative results</h3>
            <p>Conclude the project report with final remarks.</p>
        </section>

        <section id="section3-2" class="subtitle">
            <h3>3.2 Quantitative results</h3>
            <table>
                <tr>
                    <th>Algorithm/Optimizer</th>
                    <th>Runtime(Seconds)</th>
                    <th>Loss</th>
                    <th>Accuracy(%)</th>
                </tr>
                <tr>
                    <td>Adam</td>
                    <td>09 - 14</td>
                    <td>0.35 - 0.58</td>
                    <td>85 - 93</td>
                </tr>
                <tr>
                    <td>SGD</td>
                    <td>08 - 09</td>
                    <td>0.49 - 0.72</td>
                    <td>63 - 80</td>
                </tr>
                <tr>
                    <td>RMSprop</td>
                    <td>08 - 10</td>
                    <td>0.33 - 0.74</td>
                    <td>74 - 89</td>
                </tr>
                <tr>
                    <td>AdaGrad</td>
                    <td>08 - 10</td>
                    <td>0.23 - 0.58</td>
                    <td>74 - 91</td>
                </tr>
            </table>
            <br>
            <p><b>Adam:</b> is a popular optimizer known for its efficiency and quick convergence. 
                It performs well on a wide range of tasks and is often a good default choice.</p>
            <p><b>SGD:</b> is a classical optimizer that can be effective with appropriate learning rate tuning. 
                It might be suitable for smaller datasets or when fine-tuning is necessary.</p>
            <p><b>RMSprop:</b> is another adaptive learning rate optimizer that is suitable for non-stationary problems. 
                It often performs well in scenarios with varying gradients.</p>
            <p><b>Adagrad:</b> adapts the learning rates for each parameter individually based on the historical gradients.
                It's particularly suitable for sparse data and can be beneficial in scenarios where certain features have significantly different frequencies</p>
        </section>
    </section>

    <section id="section4" class="title">
        <h1>4 Discussion</h1>

        <section id="section4-1" class="subtitle">
            <h3>4.1 Limitations of the work and directions for future work</h3>
            <p>Conclude the project report with final remarks.</p>
        </section>

        <section id="section4-2" class="subtitle">
            <h3>4.2 Implications of the work</h3>
            <p>
                Looking forward, Project Dory envisions a future where the AI platform goes beyond mere sound classification. 
                The goal is to extend its capabilities to identify the geographic locations of marine mammals, providing a holistic understanding of their distribution across the oceans. 
                This geographical insight holds immense potential for marine conservation efforts, enabling researchers to identify critical habitats and migration patterns.

            </p>
            <p>
                Crucially, this geographic identification feature can also be leveraged to benefit various human activities in the oceans. 
                By integrating the AI into marine traffic management systems, Project Dory aims to assist fishermen, cargo ships, and other maritime endeavors in avoiding accidental encounters with marine animals. 
                This proactive approach not only safeguards marine life from unintended harm but also enhances the safety and efficiency of human activities in the vast expanse of the ocean.

            </p>
        </section>
    </section>

    <section id="section5" class="title">
        <h1>5 Statement of contributions</h1>
        <p>Conclude the project report with final remarks.</p>
    </section>

    <section id="section6" class="title">
        <h1>6 References</h1>
        <p>
            Joshi, S. (2021, March 25). Best of watkins marine mammal sound database. Kaggle. https://www.kaggle.com/datasets/shreyj1729/best-of-watkins-marine-mammal-sound-database
        </p>
        <p>
            Martinpiotte. (2018, July 25). Whale recognition model with score 0.78563. Kaggle. https://www.kaggle.com/code/martinpiotte/whale-recognition-model-with-score-0-78563/notebook
        </p>
        <p>
            Sanctsound: Studying the underwater world of sound. National Centers for Environmental Information (NCEI). (2023, February 8). https://www.ncei.noaa.gov/news/sanctsound-studying-underwater-world-sound
        </p>
        <p>
            Watkins Marine Mammal Sound Database. (n.d.). https://cis.whoi.edu/science/B/whalesounds/index.cfm 
        </p>
        
    </section>

</body>

</html>
